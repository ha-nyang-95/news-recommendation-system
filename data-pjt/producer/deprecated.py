import feedparser
import requests
import os
from bs4 import BeautifulSoup
import psycopg2
import json
from datetime import datetime
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer

# .env íŒŒì¼ì— ì €ì¥ëœ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# PostgreSQL ì—°ê²° ì •ë³´
DB_CONFIG = {
    "dbname": "news",
    "user": os.getenv("DB_USERNAME"),
    "password": os.getenv("DB_PASSWORD"),
    "host": "localhost",
    "port": 5432
}

# ë‰´ìŠ¤ RSS í”¼ë“œ ì£¼ì†Œ (ë§¤ì¼ê²½ì œ ì‚¬íšŒë©´)
RSS_FEED_URL = "https://www.mk.co.kr/rss/30100041/"

# ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (Ko-SBERT)
embedding_model = SentenceTransformer("jhgan/ko-sbert-nli")

# ğŸ” ë‰´ìŠ¤ ë³¸ë¬¸ê³¼ ê¸°ìëª… í¬ë¡¤ë§
def extract_article_text(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")

        content = ""
        wrapper = soup.find("div", class_="news_cnt_detail_wrap")
        if wrapper:
            paragraphs = wrapper.find_all("p")
            if paragraphs:
                content = "\n\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        else:
            figcaption = soup.select_one("figure figcaption")
            if figcaption:
                content = figcaption.get_text(strip=True)

        if not content:
            content = "[ë³¸ë¬¸ ì—†ìŒ]"

        author_tags = soup.select("dl.author > a")
        writers = [tag.get_text(strip=True) for tag in author_tags]
        writer = ", ".join(writers) if writers else "ì•Œ ìˆ˜ ì—†ìŒ"

        return content, writer

    except Exception as e:
        print(f"[ë³¸ë¬¸/ê¸°ì í¬ë¡¤ë§ ì˜¤ë¥˜] {url} â†’ {e}")
        return "[ë³¸ë¬¸ ì—†ìŒ]", "ì•Œ ìˆ˜ ì—†ìŒ"

# ğŸ§  TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, top_n=5):
    try:
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.toarray()[0]
        keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:top_n]
        return [word for word, _ in keywords]
    except Exception as e:
        print(f"[í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜] {e}")
        return []

# ğŸ§¬ ë¬¸ì¥ ì„ë² ë”© ìƒì„±
def generate_embedding(text):
    try:
        return embedding_model.encode(text).tolist()  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
    except Exception as e:
        print(f"[ì„ë² ë”© ì˜¤ë¥˜] {e}")
        return None

# ğŸ—‚ PostgreSQLì— ê¸°ì‚¬ ì €ì¥
def save_article_to_db(title, writer, pub_date, category, content, link):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        # ì¤‘ë³µ ì²´í¬
        cur.execute("SELECT 1 FROM news_article WHERE url = %s", (link,))
        if cur.fetchone():
            print(f"[ì¤‘ë³µ] ì´ë¯¸ ì¡´ì¬: {title}")
        else:
            keywords = extract_keywords(content)
            embedding = generate_embedding(content)

            cur.execute(
                """
                INSERT INTO news_article 
                (title, writer, write_date, category, content, url, keywords, embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    title,
                    writer,
                    pub_date,
                    category,
                    content,
                    link,
                    json.dumps(keywords, ensure_ascii=False),
                    embedding
                )
            )
            conn.commit()
            print(f"[ì €ì¥ ì™„ë£Œ] {title}")

        cur.close()
        conn.close()

    except Exception as e:
        print(f"[DB ì €ì¥ ì˜¤ë¥˜] {title} â†’ {e}")


# ğŸ›  ê¸°ì¡´ null embedding ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_missing_embeddings():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        cur.execute("SELECT id, content FROM news_article WHERE embedding IS NULL")
        rows = cur.fetchall()
        print(f"[ì—…ë°ì´íŠ¸ ëŒ€ìƒ] NULL embedding {len(rows)}ê±´")

        for id_, content in rows:
            try:
                embedding = embedding_model.encode(content).tolist()
                cur.execute(
                    "UPDATE news_article SET embedding = %s WHERE id = %s",
                    (embedding, id_)
                )
                print(f"[ì—…ë°ì´íŠ¸ ì™„ë£Œ] ID: {id_}")
            except Exception as e:
                print(f"[ì„ë² ë”© ìƒì„± ì‹¤íŒ¨] ID: {id_} â†’ {e}")

        conn.commit()
        cur.close()
        conn.close()
        print("[âœ… ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ]")

    except Exception as e:
        print(f"[DB ì—°ê²° ì˜¤ë¥˜] {e}")


# ğŸ” ì „ì²´ ì‹¤í–‰ ë¡œì§
def main():
    print("ğŸ“° RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘")
    feed = feedparser.parse(RSS_FEED_URL)

    for entry in feed.entries:
        title = entry.title
        link = entry.link
        pub_date = entry.get('published_parsed')
        pub_date = datetime(*pub_date[:6]) if pub_date else datetime.now()
        category = entry.get("category", "ê¸°íƒ€")

        content, writer = extract_article_text(link)
        if content and content != "[ë³¸ë¬¸ ì—†ìŒ]":
            save_article_to_db(title, writer, pub_date, category, content, link)
    
    print("ğŸ“Œ ê¸°ì¡´ ëˆ„ë½ëœ embedding ì—…ë°ì´íŠ¸ ì‹œì‘")
    update_missing_embeddings()

if __name__ == "__main__":
    main()
